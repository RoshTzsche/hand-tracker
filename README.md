# Rosh Multimodal Tracker ğŸ–ï¸ğŸ™‚

Un sistema de interacciÃ³n multimodal avanzado basado en visiÃ³n por computadora que combina el rastreo de manos (Hand Tracking) y el reconocimiento de expresiones faciales (Face Mesh) para activar eventos y superposiciones visuales en tiempo real.

Este proyecto utiliza **MediaPipe** para la inferencia geomÃ©trica y **OpenCV** para el procesamiento de imÃ¡genes, diseÃ±ado para funcionar eficientemente en entornos Linux (especÃ­ficamente optimizado para Fedora/Hyprland).

## ğŸš€ CaracterÃ­sticas Principales

* **DetecciÃ³n Multimodal SimultÃ¡nea:** Rastrea manos y rostro al mismo tiempo sin pÃ©rdida significativa de rendimiento.
* **Sistema de "Combos":** Una arquitectura lÃ³gica que mapea pares de `(Gesto Mano, ExpresiÃ³n Facial)` a acciones especÃ­ficas.
    * *Ejemplo:* Un "Pulgar Arriba" con una "Sonrisa" genera un overlay diferente a un "Pulgar Arriba" con rostro "Neutral".
* **Feedback Visual en Tiempo Real:** SuperposiciÃ³n de imÃ¡genes (overlays) con soporte de transparencia (Canal Alpha/BGRA).
* **ClasificaciÃ³n GeomÃ©trica Personalizada:** Algoritmos propios para determinar estados como "Sorpresa" o "GuiÃ±o" basados en distancias euclidianas y proporciones faciales.

## ğŸ› ï¸ Requisitos del Sistema

* **Sistema Operativo:** Linux (Probado en Fedora 42 con Hyprland).
* **Python:** VersiÃ³n 3.8 a 3.11.
    * *Nota importante:* El proyecto fue desarrollado y validado en **Python 3.11**. Versiones superiores (3.12+) presentan incompatibilidades con algunas dependencias (especÃ­ficamente `mediapipe`/`distutils`) a fecha de Noviembre 2025.
* **Hardware:** Webcam funcional.

## ğŸ“¦ InstalaciÃ³n

Sigue estos pasos para configurar el entorno desde cero:

### 1. Clonar el Repositorio
```bash
git clone [https://github.com/tu-usuario/hand-tracker.git](https://github.com/tu-usuario/hand-tracker.git)
cd hand-tracker
````

### 2\. Crear Entorno Virtual (Recomendado)

Para mantener las dependencias aisladas de tu sistema principal (Fedora):

```bash
python3 -m venv venv_gestos
source venv_gestos/bin/activate
```

### 3\. Instalar Dependencias

Instala las librerÃ­as necesarias ejecutando:

```bash
pip install opencv-python mediapipe numpy matplotlib
```

### 4\. âš ï¸ ConfiguraciÃ³n de Recursos (CRÃTICO)

El sistema requiere una carpeta especÃ­fica para los recursos grÃ¡ficos que **no estÃ¡ incluida en el repositorio** por defecto. Debes crearla manualmente y aÃ±adir tus imÃ¡genes.

1.  Crea la carpeta `images` en la raÃ­z del proyecto:

    ```bash
    mkdir images
    ```

2.  AÃ±ade archivos `.png` dentro de esa carpeta. Para que el sistema funcione, los nombres de archivo deben coincidir con los definidos a continuaciÃ³n (o puedes modificar las rutas en `actions.py`). AsegÃºrate de tener las siguientes imÃ¡genes:

      * **BÃ¡sicos:** `like.png`, `dislike.png`, `rock.png`, `peace.png`
      * **Emociones:** `shocked.png`, `look_there.png`, `party.png`
      * **Positividad:** `super_like.png`, `hello.png`, `happy_vibes.png`, `idea.png`
      * **GuiÃ±os:** `secret.png`, `target_locked.png`, `bro_fist.png`, `high_five.png`

> **Nota:** El sistema normalizarÃ¡ automÃ¡ticamente las imÃ¡genes a formato BGRA y las redimensionarÃ¡, pero es recomendable usar imÃ¡genes PNG con fondo transparente para un mejor efecto visual.

## ğŸ“ Fundamentos TÃ©cnicos (Desglose MatemÃ¡tico)

El nÃºcleo de la clasificaciÃ³n no depende de redes neuronales de "caja negra" para la clasificaciÃ³n final, sino de **geometrÃ­a analÃ­tica** aplicada sobre los *landmarks* extraÃ­dos por MediaPipe.

### 1\. ClasificaciÃ³n de Manos (LÃ³gica Vectorial)

Para determinar si un dedo estÃ¡ levantado, no usamos aprendizaje profundo, sino la comparaciÃ³n de distancias euclidianas cuadrÃ¡ticas ($d^2$) para evitar el costo computacional de las raÃ­ces cuadradas en cada frame.

Sea $P_{wrist}$ la muÃ±eca, $P_{tip}$ la punta del dedo y $P_{pip}$ la articulaciÃ³n intermedia:
$$d^2(P_{wrist}, P_{tip}) > d^2(P_{wrist}, P_{pip}) \implies \text{Dedo Levantado}$$

### 2\. DetecciÃ³n de Sorpresa (MAR - Mouth Aspect Ratio)

Para detectar una boca abierta (sorpresa), calculamos la relaciÃ³n de aspecto de la boca utilizando la distancia euclidiana:

$$MAR = \frac{||P_{top} - P_{bottom}||}{||P_{left} - P_{right}||}$$

Donde $|| \cdot ||$ es la norma euclidiana. Si $MAR > 0.45$, se clasifica como `SURPRISED`.

### 3\. DetecciÃ³n de GuiÃ±os (EAR - Eye Aspect Ratio)

Utilizamos la mÃ©trica estÃ¡ndar EAR para determinar la apertura del ojo. Se consideran 6 puntos de referencia por ojo ($p_1 \dots p_6$):

$$EAR = \frac{||p_2 - p_6|| + ||p_3 - p_5||}{2 \cdot ||p_1 - p_4||}$$

El sistema detecta un guiÃ±o intencional comparando los EAR de ambos ojos:

$$ \text{Si} (EAR_{left} < 0.2 \land EAR_{right} > 0.2) \implies \text{WINK\_LEFT}$$

## ğŸ® Uso

Para iniciar el sistema principal de rastreo:

```bash
python tracker.py
```

### Controles

  * **ESC:** Cerrar la ventana y terminar el programa.

## âš™ï¸ ConfiguraciÃ³n Avanzada

### SelecciÃ³n de CÃ¡mara

El archivo `tracker.py` intenta localizar una cÃ¡mara especÃ­fica por su ID de hardware (`/dev/v4l/by-id/...`) para evitar problemas en sistemas con mÃºltiples dispositivos de video en Linux.

Si tu cÃ¡mara no es detectada, edita la lÃ­nea en `tracker.py`:

```python
# Cambia esto por el Ã­ndice de tu cÃ¡mara (generalmente 0 o 1)
stable_path = "/ruta/a/tu/camara" 
# O fuerza el Ã­ndice directamente en cv2.VideoCapture(0)
```

## ğŸ“‚ Estructura del Proyecto

```text
hand-tracker/
â”œâ”€â”€ actions.py       # Controlador de lÃ³gica de combos y carga de imÃ¡genes
â”œâ”€â”€ tracker.py       # Punto de entrada principal (Loop de visiÃ³n)
â”œâ”€â”€ images/          # [TÃš DEBES CREAR ESTO] Carpeta de recursos PNG
â”œâ”€â”€ .gitignore       # ConfiguraciÃ³n de exclusiÃ³n de git
â””â”€â”€ README.md        # DocumentaciÃ³n
```

## ğŸ¤ ContribuciÃ³n

Si deseas agregar nuevos combos, edita el diccionario `self.combo_map` en `actions.py` y aÃ±ade la imagen correspondiente en la carpeta `images/`.

```python
# Ejemplo de nuevo combo
("FIST", "SMILE"): "./images/power_up.png",
```

-----

*Desarrollado por Rosh.*

